{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 REPORT\n",
    "\n",
    "## SGD with momentum\n",
    "Adding momentum to SGD is like adding inertia (mass) to the gradient descent process. The momentum carries history information and helps prevent the fluctuaion of stocastic gradient. \n",
    "\n",
    "With $\\mu < 1$, the old information of gradient will decay exponentially, so the weight of recent gradients is larger than old ones.\n",
    "\n",
    "I use the following fomular:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\mu * m_{t-1} + (1-\\mu)*\\eta * g_t \\\\\n",
    "p_t &= p_{t-1} - m_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Notice that I apply an additional $(1-\\mu)$ on the gradient $g_t$. To get the same effect with the usual version\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\mu * m_{t-1} + g_t \\\\\n",
    "p_t &= p_{t-1} - \\eta' * m_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "we need to set $\\eta=\\eta'\\big/(1-\\mu)$\n",
    "\n",
    "## Adam\n",
    "Adam algorithm uses the second moment of gradient to control the step size. In addition to the momentum, we also need to store the square of gradient for each learnable parameter.\n",
    "\n",
    "I use the following fomular:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\beta_1 * m_{t-1} + (1-\\beta_1) * g_t \\\\\n",
    "v_t &= \\beta_2 * v_{t-1} + (1-\\beta_2) * g_t^2\\\\\n",
    "p_t &= p_{t-1} - \\eta * \\frac{m_t}{\\sqrt{v_t+\\epsilon}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\eta$ is the learning rate. All operations are element-wise for vector $m_t,\\,v_t$ and $g_t$.\n",
    "\n",
    "------\n",
    "\n",
    "## Results\n",
    "### One-neuron model\n",
    "Figure 1: comparison of three algorithms with $\\eta=1\\times 10^{-3}$\n",
    "\n",
    "![one neuron 1](./plots/output_one_neuron1.png)\n",
    "\n",
    "Other parameters are the same as below:\n",
    "```python\n",
    "mp3 = ModifiedPrimer(\n",
    "        one_neuron_model = True,\n",
    "        expressions = ['xw=ab*xa+bc*xb+cd*xc+ac*xd'],\n",
    "        output_vars = ['xw'],\n",
    "        dataset_size = 5000,\n",
    "        learning_rate = 1e-3,\n",
    "        rate_mu = 0.95      ## for SGD+\n",
    "        rate_beta1 = 0.9,   ## for Adam\n",
    "        rate_beta2 = 0.99,  ## for Adam\n",
    "        training_iterations = 240000,\n",
    "        batch_size = 8,\n",
    "        display_loss_how_often = 200,\n",
    "        debug = True,\n",
    ")\n",
    "```\n",
    "\n",
    "Now I change learning rate $\\eta=4\\times 10^{-3}$.\n",
    "\n",
    "Figure 2:\n",
    "\n",
    "![one neuron 2](./plots/output_one_neuron2.png)\n",
    "\n",
    "I also want to make the batch size larger. \n",
    "\n",
    "By setting `batch_size = 8` I get\n",
    "\n",
    "Figure 3:\n",
    "\n",
    "![one neuron 3](./plots/output_one_neuron3.png)\n",
    "\n",
    "\n",
    "### Multi-neuron model\n",
    "\n",
    "For this model I use\n",
    "```python\n",
    "mp1 = ModifiedPrimer(\n",
    "        num_layers = 3,\n",
    "        layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "        expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                        'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                        'xo=cp*xw+cq*xz'],\n",
    "        output_vars = ['xo'],\n",
    "        dataset_size = 5000,\n",
    "        learning_rate = 1e-3,\n",
    "        rate_mu = 0.95,\n",
    "        rate_beta1 = 0.9,\n",
    "        rate_beta2 = 0.99,\n",
    "        training_iterations = 40000,\n",
    "        batch_size = 8,\n",
    "        display_loss_how_often = 100,\n",
    "        debug = True,\n",
    ")\n",
    "```\n",
    "\n",
    "The result with $\\eta=1\\times 10^{-3}$ is listed below.\n",
    "\n",
    "Figure 4:\n",
    "\n",
    "![multi neuron 1](./plots/output_multi_neuron.png)\n",
    "\n",
    "I found that the Adam algorithm is still speeding up. The step size may be too small.\n",
    "\n",
    "If I change the learning rate to $\\eta=1\\times 10^{-2}$, I get\n",
    "\n",
    "Figure 5:\n",
    "\n",
    "![multi neuron 2](./plots/output_multi_neuron2.png)\n",
    "\n",
    "------\n",
    "\n",
    "## Discussion\n",
    "\n",
    "I found that the Adam algorithm outperforms the other two in most cases (with same step size).\n",
    "\n",
    "Given enough steps Adam and SGD+ may reach the same loss around $L=0.15$, but SGD ususlly stops above $0.15$.\n",
    "I guess that algorithms using the momentum term $m_t = \\mu m_{t-1} + (1-\\mu)\\eta g_t$ makes them converge faster: Consider we are around $L = L_{\\text{min}}$ in parameter space, the total gradient $g \\to 0$, but the stochastic gradient are always fluctuating. $m_t$ is kind of mean of multiple $g_t$, so it should be closer to $0$. \n",
    "\n",
    "There are still some parameters to change: $\\mu$ and $\\beta1,\\,\\beta2$. By changing $\\mu$ we may find its best value for SGD+, like this:\n",
    "\n",
    "Figure 6:\n",
    "\n",
    "![mu](./output.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20140d5a202b5d660d65448b53023de7dbe126891fb42d4643a40fe5963133e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
