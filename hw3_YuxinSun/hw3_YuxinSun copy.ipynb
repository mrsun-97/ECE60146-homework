{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch_directml\n",
    "# dml = torch_directml.device()\n",
    "# tensor1 = torch.tensor([1]).to(dml) # Note that dml is a variable, not a string!\n",
    "# tensor2 = torch.tensor([2]).to(dml)\n",
    "# dml_algebra = tensor1 + tensor2\n",
    "# dml_algebra.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys,os,os.path\n",
    "import numpy as np\n",
    "import re\n",
    "import operator\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ComputationalGraphPrimer import *\n",
    "\n",
    "class ModifiedPrimer(ComputationalGraphPrimer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs ):\n",
    "        rate_mu = rate_beta1 = rate_beta2 = None\n",
    "        if 'rate_mu' in kwargs:   rate_mu = kwargs.pop('rate_mu')\n",
    "        if 'rate_beta1' in kwargs:  rate_beta1 = kwargs.pop('rate_beta1')\n",
    "        if 'rate_beta2' in kwargs:  rate_beta2 = kwargs.pop('rate_beta2')\n",
    "        if 'rate_ep' in kwargs:   rate_ep = kwargs.pop('rate_ep')\n",
    "        if rate_mu:\n",
    "            self.rate_mu = rate_mu\n",
    "        else:\n",
    "            self.rate_mu = 0.9\n",
    "        if rate_beta1:\n",
    "            self.rate_beta1 = rate_beta1\n",
    "        else:\n",
    "            self.rate_beta1 = 0.9\n",
    "        if rate_beta2:\n",
    "            self.rate_beta2 = rate_beta2\n",
    "        else:\n",
    "            self.rate_beta2 = 0.99\n",
    "        self.epsilon = 1e-8\n",
    "        self.loss_running_record = []\n",
    "        super().__init__(*args, **kwargs )\n",
    "\n",
    "    def run_training_loop_one_neuron_model(self, training_data, *, algo=\"SGD\", show_process=False):\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = random.uniform(0,1)                   ## Adding the bias improves class discrimination.\n",
    "                                                          ##   We initialize it to a random number.\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]   ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]   ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data\n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "        # list of available optimizer\n",
    "        algos = {\n",
    "            \"SGD\":  self.backprop_and_update_params_one_neuron_model,\n",
    "            \"SGD+\": self.backprop_and_update_params_one_neuron_model_momentum,\n",
    "            \"Adam\": self.backprop_and_update_params_one_neuron_model_adam,\n",
    "        }\n",
    "\n",
    "        backprop = None\n",
    "        if algo in algos: \n",
    "            backprop = algos[algo]\n",
    "        else:\n",
    "            raise ValueError(\"algorithm does not exist\\n\")\n",
    "\n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                    ##  Average the loss over iterations for printing out \n",
    "                                                                           ##    every N iterations during the training loop.\n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            y_preds, deriv_sigmoids =  self.forward_prop_one_neuron_model(data_tuples)              ##  FORWARD PROP of data\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ##  Find loss\n",
    "            loss_avg = loss / float(len(class_labels))                                              ##  Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                          \n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                # print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))                 ## Display average loss\n",
    "                avg_loss_over_iterations = 0.0                                                     ## Re-initialize avg loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            deriv_sigmoid_avg = sum(deriv_sigmoids) / float(len(class_labels))\n",
    "            data_tuple_avg = [sum(x) for x in zip(*data_tuples)]\n",
    "            data_tuple_avg = list(map(operator.truediv, data_tuple_avg, \n",
    "                                     [float(len(class_labels))] * len(class_labels) ))\n",
    "            backprop(y_error_avg, data_tuple_avg, deriv_sigmoid_avg)     ## BACKPROP loss\n",
    "        self.loss_running_record = loss_running_record  ## add this for convenience\n",
    "        if show_process:\n",
    "            plt.figure()     \n",
    "            plt.plot(loss_running_record)\n",
    "            plt.show()  \n",
    "\n",
    "    def run_training_loop_multi_neuron_model(self, training_data, algo=\"SGD\", show_process=False):\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "\n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        # list of available optimizer\n",
    "        algos = {\n",
    "            \"SGD\":  self.backprop_and_update_params_multi_neuron_model,\n",
    "            \"SGD+\": self.backprop_and_update_params_multi_neuron_model_momentum,\n",
    "            \"Adam\": self.backprop_and_update_params_multi_neuron_model_adam,\n",
    "        }\n",
    "\n",
    "        backprop = None\n",
    "        if algo in algos: \n",
    "            backprop = algos[algo]\n",
    "        else:\n",
    "            raise ValueError(\"algorithm does not exist\\n\")\n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                # print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            backprop(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "        self.loss_running_record = loss_running_record  ## add this for convenience\n",
    "        if show_process:\n",
    "            plt.figure()     \n",
    "            plt.plot(loss_running_record)\n",
    "            plt.show()  \n",
    "\n",
    "    def backprop_and_update_params_one_neuron_model_momentum(self, y_error, vals_for_input_vars, deriv_sigmoid,\\\n",
    "        *, \\\n",
    "        vals_momentum={}):\n",
    "\n",
    "        input_vars = self.independent_vars\n",
    "        input_vars_to_param_map = self.var_to_var_param[self.output_vars[0]]\n",
    "        param_to_vars_map = {param : var for var, param in input_vars_to_param_map.items()}\n",
    "        vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "        vals_for_learnable_params = self.vals_for_learnable_params\n",
    "\n",
    "        if vals_momentum == {}:\n",
    "            ## for the first back-propagation we initialize the momentum as the first gradient,\n",
    "            ## this way we can avoid the bias of momentum to zero.\n",
    "            for i,param in enumerate(self.vals_for_learnable_params):\n",
    "                ## Calculate the next step in the parameter hyperplane\n",
    "                step = self.learning_rate * y_error * vals_for_input_vars_dict[param_to_vars_map[param]] * deriv_sigmoid    \n",
    "                vals_momentum[param] = step\n",
    "                ## Update the learnable parameters\n",
    "                self.vals_for_learnable_params[param] += step\n",
    "            # self.bias += self.learning_rate * y_error * deriv_sigmoid    ## Update the bias\n",
    "            bias = self.learning_rate * y_error * deriv_sigmoid\n",
    "            vals_momentum[\"_b\"] = bias\n",
    "            self.bias += bias\n",
    "        else:\n",
    "            for i,param in enumerate(self.vals_for_learnable_params):\n",
    "                ## Calculate the next step in the parameter hyperplane\n",
    "                step = self.learning_rate * y_error * vals_for_input_vars_dict[param_to_vars_map[param]] * deriv_sigmoid    \n",
    "                vals_momentum[param] = self.rate_mu*vals_momentum[param] + (1-self.rate_mu)*step\n",
    "                ## Update the learnable parameters\n",
    "                self.vals_for_learnable_params[param] += vals_momentum[param]\n",
    "            ## Update the bias\n",
    "            bias = self.learning_rate * y_error * deriv_sigmoid\n",
    "            vals_momentum[\"_b\"] = self.rate_mu*vals_momentum[\"_b\"] + (1-self.rate_mu)*bias\n",
    "            self.bias += vals_momentum[\"_b\"]\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model_momentum(self, y_error, class_labels, \\\n",
    "        *, \\\n",
    "        vals_momentum={}):\n",
    "\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        is_first_sweep = True\n",
    "        if vals_momentum != {}:\n",
    "            is_first_sweep = False\n",
    "\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            if is_first_sweep:\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j] \n",
    "                        step = self.learning_rate * gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                        vals_momentum[param] = step ## push to dict\n",
    "                        self.vals_for_learnable_params[param] += step\n",
    "                bias = self.learning_rate * sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                            * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)\n",
    "                if \"_b\" not in vals_momentum:\n",
    "                    ## create momentum of bias for each layer\n",
    "                    vals_momentum[\"_b\"] = [0.0 for i in range(self.num_layers-1)]\n",
    "                vals_momentum[\"_b\"][back_layer_index-1] = bias\n",
    "                self.bias[back_layer_index-1] += bias\n",
    "            else:\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j] \n",
    "                        step = self.learning_rate * gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                        vals_momentum[param] = self.rate_mu*vals_momentum[param] + (1-self.rate_mu)*step ## push to dict\n",
    "                        self.vals_for_learnable_params[param] += vals_momentum[param]\n",
    "                bias = self.learning_rate * sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                            * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)\n",
    "                vals_momentum[\"_b\"][back_layer_index-1] = self.rate_mu*vals_momentum[\"_b\"][back_layer_index-1] \\\n",
    "                                                                            + (1-self.rate_mu)*bias\n",
    "                self.bias[back_layer_index-1] += vals_momentum[\"_b\"][back_layer_index-1]\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_one_neuron_model_adam(self, y_error, vals_for_input_vars, deriv_sigmoid,\\\n",
    "        *, \\\n",
    "        vals_momentum={}, vals_variance={}):\n",
    "\n",
    "        input_vars = self.independent_vars\n",
    "        input_vars_to_param_map = self.var_to_var_param[self.output_vars[0]]\n",
    "        param_to_vars_map = {param : var for var, param in input_vars_to_param_map.items()}\n",
    "        vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "        vals_for_learnable_params = self.vals_for_learnable_params\n",
    "\n",
    "        if vals_momentum == {}:\n",
    "            ## for the first back-propagation we initialize the momentum as the first gradient,\n",
    "            ## this way we can avoid the bias of momentum to zero.\n",
    "            for i,param in enumerate(self.vals_for_learnable_params):\n",
    "                ## Calculate the next step in the parameter hyperplane\n",
    "                step = y_error * vals_for_input_vars_dict[param_to_vars_map[param]] * deriv_sigmoid    \n",
    "                vals_momentum[param] = step\n",
    "                vals_variance[param] = step**2\n",
    "                ## Update the learnable parameters\n",
    "                self.vals_for_learnable_params[param] += self.learning_rate * step / np.sqrt(step**2 + self.epsilon)\n",
    "            # self.bias += self.learning_rate * y_error * deriv_sigmoid    ## Update the bias\n",
    "            bias = y_error * deriv_sigmoid\n",
    "            vals_momentum[\"_b\"] = bias\n",
    "            vals_variance['_b'] = bias**2\n",
    "            self.bias += self.learning_rate * bias / np.sqrt(bias**2 + self.epsilon)\n",
    "        else:\n",
    "            for i,param in enumerate(self.vals_for_learnable_params):\n",
    "                ## Calculate the next step in the parameter hyperplane\n",
    "                step = y_error * vals_for_input_vars_dict[param_to_vars_map[param]] * deriv_sigmoid    \n",
    "                vals_momentum[param] = self.rate_beta1*vals_momentum[param] + (1-self.rate_beta1)*step\n",
    "                vals_variance[param] = self.rate_beta2*vals_variance[param] + (1-self.rate_beta2)*step**2\n",
    "                ## Update the learnable parameters\n",
    "                self.vals_for_learnable_params[param] += self.learning_rate * vals_momentum[param] \\\n",
    "                                                                    / np.sqrt(vals_variance[param]+self.epsilon)\n",
    "            ## Update the bias\n",
    "            bias = y_error * deriv_sigmoid\n",
    "            vals_momentum[\"_b\"] = self.rate_beta1*vals_momentum[\"_b\"] + (1-self.rate_beta1)*bias\n",
    "            vals_variance[\"_b\"] = self.rate_beta2*vals_variance[\"_b\"] + (1-self.rate_beta2)*bias**2\n",
    "            self.bias += self.learning_rate * vals_momentum[\"_b\"] / np.sqrt(vals_variance[\"_b\"] + self.epsilon)\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model_adam(self, y_error, class_labels, \\\n",
    "        *, \\\n",
    "        vals_momentum={}, vals_variance={}):\n",
    "\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        is_first_sweep = True\n",
    "        if vals_momentum != {}:\n",
    "            is_first_sweep = False\n",
    "\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            if is_first_sweep:\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j] \n",
    "                        step = gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                        vals_momentum[param] = step ## push to dict\n",
    "                        vals_variance[param] = step**2\n",
    "                        # self.vals_for_learnable_params[param] += step\n",
    "                        self.vals_for_learnable_params[param] += self.learning_rate * step / np.sqrt(step**2 + self.epsilon)\n",
    "                bias = sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                    * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)\n",
    "                if \"_b\" not in vals_momentum:\n",
    "                    ## create momentum of bias for each layer\n",
    "                    vals_momentum[\"_b\"] = [0.0 for i in range(self.num_layers-1)]\n",
    "                    vals_variance['_b'] = [0.0 for i in range(self.num_layers-1)]\n",
    "                vals_momentum[\"_b\"][back_layer_index-1] = bias\n",
    "                vals_variance[\"_b\"][back_layer_index-1] = bias**2\n",
    "                self.bias[back_layer_index-1] += self.learning_rate * bias / np.sqrt(bias**2 + self.epsilon)\n",
    "            else:\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j] \n",
    "                        step = gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                        vals_momentum[param] = self.rate_beta1*vals_momentum[param] + (1-self.rate_beta1)*step ## push to dict\n",
    "                        vals_variance[param] = self.rate_beta2*vals_variance[param] + (1-self.rate_beta2)*step**2\n",
    "                        self.vals_for_learnable_params[param] += self.learning_rate * vals_momentum[param] \\\n",
    "                                                                            / np.sqrt(vals_variance[param] + self.epsilon)\n",
    "                bias = self.learning_rate * sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                            * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)\n",
    "                vals_momentum[\"_b\"][back_layer_index-1] = self.rate_beta1*vals_momentum[\"_b\"][back_layer_index-1] \\\n",
    "                                                                        + (1-self.rate_beta1)*bias\n",
    "                vals_variance[\"_b\"][back_layer_index-1] = self.rate_beta2*vals_variance[\"_b\"][back_layer_index-1] \\\n",
    "                                                                        + (1-self.rate_beta2)*bias**2\n",
    "                self.bias[back_layer_index-1] += self.learning_rate * vals_momentum[\"_b\"][back_layer_index-1] \\\n",
    "                                                            / np.sqrt(vals_variance[\"_b\"][back_layer_index-1] + self.epsilon)\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'ar', 'bp', 'bs', 'bq', 'br', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': set(), 'xw': set(), 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xo', 'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'ar', 'bp', 'bs', 'cq', 'bq', 'br', 'cp', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xo': set(), 'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': {'xo'}, 'xw': {'xo'}, 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a100785b0>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a100789d0>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a10078820>]}\n",
      "SGD done\n"
     ]
    }
   ],
   "source": [
    "mp1 = ModifiedPrimer(\n",
    "        num_layers = 3,\n",
    "        layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "        expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                        'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                        'xo=cp*xw+cq*xz'],\n",
    "        output_vars = ['xo'],\n",
    "        dataset_size = 5000,\n",
    "        learning_rate = 1e-3,\n",
    "        # learning_rate = 1e-2,\n",
    "        training_iterations = 40000,\n",
    "        batch_size = 8,\n",
    "        display_loss_how_often = 100,\n",
    "        debug = True,\n",
    ")\n",
    "mp1.parse_multi_layer_expressions()\n",
    "# mp1.display_multi_neuron_network()   \n",
    "training_data = mp1.gen_training_data()\n",
    "mp1.run_training_loop_multi_neuron_model( training_data, algo=\"SGD\" )\n",
    "print(\"SGD done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'ar', 'bp', 'bs', 'bq', 'br', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': set(), 'xw': set(), 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xo', 'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'ar', 'bp', 'bs', 'cq', 'bq', 'br', 'cp', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xo': set(), 'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': {'xo'}, 'xw': {'xo'}, 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a10078c10>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a10078a90>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a10078af0>]}\n",
      "SGD+ done\n"
     ]
    }
   ],
   "source": [
    "mu = 0.95\n",
    "mp2 = ModifiedPrimer(\n",
    "        num_layers = 3,\n",
    "        layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "        expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                        'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                        'xo=cp*xw+cq*xz'],\n",
    "        output_vars = ['xo'],\n",
    "        dataset_size = 5000,\n",
    "        learning_rate = 1e-3 / (1-mu),\n",
    "        # learning_rate = 1e-2 / (1-mu),\n",
    "        rate_mu = mu,\n",
    "        training_iterations = 40000,\n",
    "        batch_size = 8,\n",
    "        display_loss_how_often = 100,\n",
    "        debug = True,\n",
    ")\n",
    "mp2.parse_multi_layer_expressions()\n",
    "# mp2.display_multi_neuron_network()   \n",
    "training_data = mp2.gen_training_data()\n",
    "mp2.run_training_loop_multi_neuron_model( training_data, algo=\"SGD+\" )\n",
    "print(\"SGD+ done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'ar', 'bp', 'bs', 'bq', 'br', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': set(), 'xw': set(), 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xo', 'xs', 'xq', 'xz', 'xw', 'xr', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'ar', 'bp', 'bs', 'cq', 'bq', 'br', 'cp', 'ap', 'aq', 'as'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xo': set(), 'xs': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xz': {'xo'}, 'xw': {'xo'}, 'xr': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xq', 'xs', 'xp', 'xr'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a100785e0>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a100786d0>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f6a10078640>]}\n",
      "Adam done\n"
     ]
    }
   ],
   "source": [
    "mp3 = ModifiedPrimer(\n",
    "        num_layers = 3,\n",
    "        layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "        expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                        'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                        'xo=cp*xw+cq*xz'],\n",
    "        output_vars = ['xo'],\n",
    "        dataset_size = 5000,\n",
    "        learning_rate = 1e-3,\n",
    "        # learning_rate = 1e-2,\n",
    "        rate_beta1 = 0.9,\n",
    "        rate_beta2 = 0.99,\n",
    "        training_iterations = 40000,\n",
    "        batch_size = 8,\n",
    "        display_loss_how_often = 100,\n",
    "        debug = True,\n",
    ")\n",
    "mp3.parse_multi_layer_expressions()\n",
    "# mp3.display_multi_neuron_network()   \n",
    "training_data = mp3.gen_training_data()\n",
    "mp3.run_training_loop_multi_neuron_model( training_data, algo=\"Adam\" )\n",
    "print(\"Adam done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mp1.loss_running_record, label=\"SGD\")\n",
    "plt.plot(mp2.loss_running_record, label=\"SGD+\")\n",
    "# plt.plot(l3, label=\"SGD+\")\n",
    "plt.plot(mp3.loss_running_record, label=\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20140d5a202b5d660d65448b53023de7dbe126891fb42d4643a40fe5963133e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
