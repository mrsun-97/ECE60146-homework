{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as tvt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch_directml\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import functools\n",
    "\n",
    "# use directml to run codes on AMD GPU\n",
    "dml = torch_directml.device()\n",
    "dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            if isinstance(img, Image.Image):\n",
    "                ax.imshow(img, **imshow_kwargs)\n",
    "            else:\n",
    "                ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running demo for *[]* results.\n",
      "loading annotations into memory...\n",
      "Done (t=7.45s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "annType = []\n",
    "# annType = annType[1]      #specify type here\n",
    "catType = ['airplane','bus','cat','dog','pizza']\n",
    "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
    "print('Running demo for *%s* results.'%(annType))\n",
    "dataDir='./coco'\n",
    "dataType='train2014'\n",
    "annFile = '%s/annotations/%s_%s.json'%(dataDir,prefix,dataType)\n",
    "cocoGt=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO categories: \n",
      "person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = cocoGt.loadCats(cocoGt.getCatIds())\n",
    "nms=[cat['name'] for cat in cats]\n",
    "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "cocoGt.getCatIds(catNms=['pizza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': 1,\n",
       " 'file_name': 'COCO_train2014_000000548874.jpg',\n",
       " 'coco_url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000548874.jpg',\n",
       " 'height': 480,\n",
       " 'width': 640,\n",
       " 'date_captured': '2013-11-19 21:14:26',\n",
       " 'flickr_url': 'http://farm1.staticflickr.com/139/404544387_58a31e803d_z.jpg',\n",
       " 'id': 548874}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgId = cocoGt.getImgIds(catIds=[59])[2]\n",
    "cocoGt.getAnnIds(imgIds=[imgId])\n",
    "cocoGt.loadImgs(imgId)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segmentation': [[380.32,\n",
       "   126.69,\n",
       "   379.25,\n",
       "   123.92,\n",
       "   372.86,\n",
       "   119.45,\n",
       "   371.58,\n",
       "   112.42,\n",
       "   377.33,\n",
       "   103.47,\n",
       "   383.94,\n",
       "   98.78,\n",
       "   384.58,\n",
       "   97.93,\n",
       "   395.23,\n",
       "   103.25,\n",
       "   401.62,\n",
       "   107.73,\n",
       "   403.54,\n",
       "   116.04,\n",
       "   403.11,\n",
       "   121.36,\n",
       "   400.34,\n",
       "   123.07,\n",
       "   403.33,\n",
       "   124.99,\n",
       "   405.03,\n",
       "   128.39,\n",
       "   402.69,\n",
       "   133.08,\n",
       "   399.7,\n",
       "   136.28,\n",
       "   402.47,\n",
       "   143.31,\n",
       "   399.28,\n",
       "   147.57,\n",
       "   390.97,\n",
       "   146.29,\n",
       "   385.43,\n",
       "   140.32,\n",
       "   382.66,\n",
       "   135.21,\n",
       "   381.17,\n",
       "   129.46]],\n",
       " 'area': 1039.02765,\n",
       " 'iscrowd': 0,\n",
       " 'image_id': 548874,\n",
       " 'bbox': [371.58, 97.93, 33.45, 49.64],\n",
       " 'category_id': 56,\n",
       " 'id': 1562108}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=imgId))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126.0780500000008"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imginfo = cocoGt.loadImgs(548874)[0]\n",
    "cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=imgId))[2]['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7799"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catIds = cocoGt.getCatIds(catNms=['bus', 'cat', 'pizza'])\n",
    "sets = [set(cocoGt.getImgIds(catIds=catId)) for catId in catIds]\n",
    "imgIds = functools.reduce(lambda a, b: a.union(b), sets)\n",
    "len(imgIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.84s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class DataInfo:\n",
    "    def __init__(self, dir='./coco', *, type='train2014', categories=None) -> None:\n",
    "        self.dir  = dir\n",
    "        self.type = type\n",
    "        self.annFile = '%s/annotations/%s_%s.json'%(self.dir,'instances',self.type)\n",
    "        # target images' information:\n",
    "        self.ctgs = categories\n",
    "        self.h = 256\n",
    "        self.w = 256\n",
    "        self.doArea = 40000\n",
    "\n",
    "def center_to_corner(cbox):\n",
    "    return [cbox[0], cbox[0]+cbox[2], \\\n",
    "            cbox[1], cbox[1]+cbox[3]]\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    xform = tvt.Compose([\n",
    "        tvt.ToTensor(),\n",
    "        # transform to range [-1, 1]:\n",
    "        tvt.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    def __init__(self, data: DataInfo, *, save_dir=\"./resized\", \\\n",
    "            update=False\n",
    "        ):\n",
    "\n",
    "        cocoGt   = COCO(data.annFile)\n",
    "        self.coco= cocoGt\n",
    "        catType  = data.ctgs\n",
    "        self.catIds = cocoGt.getCatIds(catNms=catType)\n",
    "        self.dir = save_dir\n",
    "        self.dataInfo = data\n",
    "        self.catId_to_label = {cocoGt.getCatIds(catType[i])[0]: i  for i in range(len(catType))}\n",
    "        self.label_to_cat = {i: catType[i] for i in range(len(catType))}\n",
    "\n",
    "        self.anns = self.gen_data_id(cocoGt, catType, update)\n",
    "\n",
    "\n",
    "    # return data and label(it is actually the categoryID)\n",
    "    def gen_data_id(self, cocoGt, catType, update=False):\n",
    "        catIds = cocoGt.getCatIds(catNms=catType)\n",
    "        sets = [set(cocoGt.getImgIds(catIds=[catId])) for catId in catIds]\n",
    "        imgIds = functools.reduce(lambda a, b: a.union(b), sets)\n",
    "        anns = []\n",
    "        for imgId in imgIds:\n",
    "            anns = cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=imgId, iscrowd=False))\n",
    "            for ann in anns:\n",
    "                if ann['category_id'] in self.catIds \\\n",
    "                and ann['area'] >= self.dataInfo.doArea:\n",
    "                    anns.append(ann['id'])\n",
    "                    self.gen_resized_image(imgId, update)\n",
    "                    # break inner for-loop\n",
    "                    break\n",
    "            # switch to next image\n",
    "        return anns\n",
    "\n",
    "    def resize(self, im, bbox):\n",
    "        w_ori = im['width']\n",
    "        h_ori = im['height']\n",
    "        # xi, yi are in range [0,1]\n",
    "        new_box = [bbox[0]/w_ori, bbox[1]/h_ori, \\\n",
    "                   bbox[2]/w_ori, bbox[3]/h_ori]\n",
    "        return new_box\n",
    "\n",
    "    def gen_resized_image(self, imgId, update):\n",
    "        im = self.coco.loadImgs(imgId)[0]\n",
    "        orig_path = '%s/%s/%s'%(self.dataInfo.dir, self.dataInfo.type, im['file_name'])\n",
    "        save_path = '%s/%s'%(self.dir, im['file_name'])\n",
    "        img = Image.open(orig_path)\n",
    "        if img.mode != 'RGB':\n",
    "            # force update if it is not RGB\n",
    "            img = img.convert('RGB')\n",
    "        if update or not os.path.exists(save_path):\n",
    "            img = img.resize((self.dataInfo.w, self.dataInfo.h), resample=Image.Resampling.LANCZOS)\n",
    "            img.save(save_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ann = self.coco.loadAnns(self.anns[index])[0]\n",
    "        im  = self.coco.loadImgs(ann['image_id'])[0]\n",
    "        path = '%s/%s/%s'%(self.dataInfo.dir, self.dataInfo.type, im['file_name'])\n",
    "        pil_image = Image.open(path)\n",
    "        tensor_img = self.xform(pil_image)\n",
    "        bbox = ann['bbox']\n",
    "        tensor_lab = self.catId_to_label[ann['category_id']] \n",
    "        new_bbox = center_to_corner(self.resize(im, bbox))\n",
    "        new_bbox = torch.tensor(new_bbox, dtype=torch.float)\n",
    "        return tensor_img, tensor_lab, new_bbox\n",
    "\n",
    "datainfo = DataInfo(type='train2014', categories=['bus', 'cat', 'pizza'])\n",
    "dataset  = MyDataset(datainfo, update=False)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ker=3, *, stride=1, padding=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, ker, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            # would have bugs if ker!=3\n",
    "            self.downsampler = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "        else:\n",
    "            self.downsampler = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsampler is not None:\n",
    "            identity = self.downsampler(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class HW5Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, out_ch=3, ngf=16):\n",
    "        super(HW5Net, self).__init__()\n",
    "\n",
    "        # The first convolution layer. Assuing (B, 3, 256, 256) to the input.\n",
    "        model = nn.ModuleList([\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_ch, ngf, 7, padding=0),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        # out_size: 256\n",
    "\n",
    "        # The second convolution layer, downsample only once before skip-block\n",
    "        model.extend([\n",
    "            nn.ReflectionPad2d(2),\n",
    "            nn.Conv2d(ngf, ngf * 2, 5, stride=3, padding=0),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        # out_size: 86\n",
    "\n",
    "        # The skip-blocks\n",
    "        new_in_ch = ngf * 2\n",
    "        num_blocks = [3, 3, 4, 2]\n",
    "        new_out_chs = [32, 64, 128, 256]\n",
    "        for i in range(len(num_blocks)):\n",
    "            new_out_ch = new_out_chs[i]\n",
    "            num_block  = num_blocks[i]\n",
    "            model.extend(\n",
    "                self._gen_skip_blocks(new_in_ch, new_out_ch, num_block, stride=2, padding=1)\n",
    "            )\n",
    "            new_in_ch = new_out_ch\n",
    "        # out_size: 6\n",
    "\n",
    "        model.extend(nn.AvgPool2d(3, stride=3, padding=0))\n",
    "        # out_size: 256x2x2\n",
    "\n",
    "        # The classification head\n",
    "        class_head = [\n",
    "            nn.Linear(256 * 2 * 2, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, out_ch),\n",
    "        ]\n",
    "        self.class_head = nn.Sequential(*class_head)\n",
    "\n",
    "        # The regression head\n",
    "        new_in_ch = new_out_chs[-1]\n",
    "        new_out_chs = [ ]\n",
    "        bbox_head = [\n",
    "\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_skip_blocks(in_ch, out_ch, num_layer, *, stride=1, padding=1):\n",
    "        # the first skip-block will downsample the input if necessary.\n",
    "        layers = [SkipBlock(in_ch, out_ch, stride=stride, padding=padding),]\n",
    "        for _ in range(1, num_layer):\n",
    "            # the following skip-blocks will keep the input size unchanged.\n",
    "            layers.append(SkipBlock(out_ch, out_ch, stride=1, padding=1))\n",
    "        return layers\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc(x, ker=3, *, stride=1, padding=1):\n",
    "    return ((x+2*padding-ker) / stride + 1.0)//1\n",
    "\n",
    "# calc(256, 5, stride=3, padding=1)\n",
    "calc(256, 5, stride=3, padding=2)\n",
    "calc(11, 3, stride=2, padding=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
