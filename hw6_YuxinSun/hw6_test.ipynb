{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as tvt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch_directml\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import os, time\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import functools\n",
    "\n",
    "# use directml to run codes on AMD GPU\n",
    "dml = torch_directml.device()\n",
    "dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInfo:\n",
    "    def __init__(self, dir='./coco', *, type='train2014', categories=None) -> None:\n",
    "        self.dir  = dir\n",
    "        self.type = type\n",
    "        self.annFile = '%s/annotations/%s_%s.json'%(self.dir,'instances',self.type)\n",
    "        # target images' information:\n",
    "        self.ctgs = categories\n",
    "        self.h = 256\n",
    "        self.w = 256\n",
    "        self.minArea = 4096\n",
    "\n",
    "    def coco_json(self):\n",
    "        return COCO(self.annFile)\n",
    "    \n",
    "yolo_interval = 8\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    xform = tvt.Compose([\n",
    "        tvt.ToTensor(),\n",
    "        # transform to range [-1, 1]:\n",
    "        tvt.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    yolo_interval = 8\n",
    "\n",
    "    def __init__(self, data_info: DataInfo, *, save_dir=\"./resized\", update=False):\n",
    "        super().__init__()\n",
    "        cocoGt = data_info.coco_json()\n",
    "        self.dir = save_dir\n",
    "        self.coco = cocoGt\n",
    "        self.data = data_info\n",
    "        self.catIds = cocoGt.getCatIds(catNms=data_info.ctgs)\n",
    "        catType = self.data.ctgs\n",
    "        self.catId_to_label = {cocoGt.getCatIds(catType[i])[0]: i  for i in range(len(catType))}\n",
    "        self.label_to_cat = {i: catType[i] for i in range(len(catType))}\n",
    "\n",
    "        self.imgIds = self.gen_data_id(update)\n",
    "\n",
    "    def gen_data_id(self, *, update=False):\n",
    "        cocoGt = self.coco\n",
    "        catIds = self.catIds\n",
    "        sets = [set(cocoGt.getImgIds(catIds=catId)) for catId in catIds]\n",
    "        imgIds = functools.reduce(lambda a, b: a.union(b), sets)\n",
    "        ids = []\n",
    "        for imgId in imgIds:\n",
    "            anns = cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=imgId, iscrowd=False))\n",
    "            for ann in anns:\n",
    "                if ann['category_id'] in self.catIds \\\n",
    "                and ann['area'] >= self.data.minArea:\n",
    "                    ids.append(imgId)\n",
    "                    self.gen_resized_image(imgId, update)\n",
    "                    # break inner for-loop\n",
    "                    break\n",
    "            # switch to next image\n",
    "        return anns\n",
    "    \n",
    "    def resize(self, im, bbox):\n",
    "        w_ori = im['width']\n",
    "        h_ori = im['height']\n",
    "        # xi, yi are in range [0,1]\n",
    "        new_box = [bbox[0]/w_ori, bbox[1]/h_ori, \\\n",
    "                   bbox[2]/w_ori, bbox[3]/h_ori]\n",
    "        return new_box\n",
    "\n",
    "    def gen_resized_image(self, imgId, update):\n",
    "        im = self.coco.loadImgs(imgId)[0]\n",
    "        orig_path = '%s/%s/%s'%(self.data.dir, self.data.type, im['file_name'])\n",
    "        save_path = '%s/%s'%(self.dir, im['file_name'])\n",
    "        img = Image.open(orig_path)\n",
    "        if img.mode != 'RGB':\n",
    "            # force update if it is not RGB\n",
    "            img = img.convert('RGB')\n",
    "        if update or not os.path.exists(save_path):\n",
    "            img = img.resize((self.data.w, self.data.h), resample=Image.Resampling.LANCZOS)\n",
    "            img.save(save_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgIds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im   = self.coco.loadImgs(self.imgIds[index])[0]\n",
    "        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=self.imgIds[index]))\n",
    "        path = '%s/%s'%(self.dir, im['file_name'])\n",
    "        pil_image = Image.open(path)\n",
    "        img_tensor = self.xform(pil_image)\n",
    "        labs_tensor = torch.zeros(5, dtype=torch.uint8) + 13\n",
    "        cell_ids = torch.zeros(5, dtype=torch.uint8)\n",
    "        anchor_ids = torch.zeros(5, dtype=torch.uint8)\n",
    "        # add one class for 'other object'\n",
    "        yolo_vectors = torch.zeros(5, 5 + len(self.catIds) + 1, dtype=torch.float)\n",
    "        i = 0\n",
    "        for ann in anns:\n",
    "            if ann['category_id'] in self.catIds \\\n",
    "            and ann['area'] >= self.data.minArea:\n",
    "                bbox_renormalized = xywh_to_ccwh(self.resize(im, anns[i]['bbox']))\n",
    "                label = self.catId_to_label[ann['category_id']]\n",
    "                cell_ids[i], dx, dy = calc_cell_id(bbox_renormalized, self.yolo_interval)\n",
    "                anchor_ids[i], sw, sh = calc_anchor_id(bbox_renormalized)\n",
    "                labs_tensor[i] = label\n",
    "                yolo_vectors[i, 0] = 1\n",
    "                yolo_vectors[i, 1:5] = torch.LongTensor([dx, dy, sw, sh])\n",
    "                yolo_vectors[i, 5+label] = 1\n",
    "\n",
    "                i += 1\n",
    "                if i >= len(anns):\n",
    "                    break\n",
    "\n",
    "        return img_tensor, labs_tensor, yolo_vectors, cell_ids, anchor_ids\n",
    "    \n",
    "def calc_cell_id(bbox, interval)->tuple(int, float, float):\n",
    "    # y-axis\n",
    "    nh = bbox[1]*interval // 1\n",
    "    if nh >= interval:\n",
    "        nh = interval - 1\n",
    "    elif nh < 0:\n",
    "        nh = 0\n",
    "    # x-axis\n",
    "    nw = bbox[0]*interval // 1\n",
    "    if nw >= interval:\n",
    "        nw = interval - 1\n",
    "    elif nw < 0:\n",
    "        nw = 0\n",
    "    return int(nh*interval + nw), bbox[0]-nw*interval-0.5, bbox[1]-nh*interval-0.5\n",
    "\n",
    "def calc_anchor_id(bbox, interval)->tuple(int, float, float):\n",
    "    wh_ratio = bbox[2] / bbox[3]\n",
    "    anchor_box_index = -1\n",
    "    rw = 0\n",
    "    rh = 0\n",
    "    if wh_ratio <= 0.25:\n",
    "        anchor_box_index = 0\n",
    "        rw = np.log(bbox[2]/1*interval)\n",
    "        rh = np.log(bbox[3]/5*interval)\n",
    "    elif wh_ratio <= 0.5:\n",
    "        anchor_box_index = 1\n",
    "        rw = np.log(bbox[2]/1*interval)\n",
    "        rh = np.log(bbox[3]/3*interval)\n",
    "    elif wh_ratio <= 2.0:\n",
    "        anchor_box_index = 2\n",
    "        rw = np.log(bbox[2]/1*interval)\n",
    "        rh = np.log(bbox[3]/1*interval)\n",
    "    elif wh_ratio <= 4.0:\n",
    "        anchor_box_index = 3\n",
    "        rw = np.log(bbox[2]/3*interval)\n",
    "        rh = np.log(bbox[3]/1*interval)\n",
    "    else:\n",
    "        anchor_box_index = 4\n",
    "        rw = np.log(bbox[2]/5*interval)\n",
    "        rh = np.log(bbox[3]/1*interval)\n",
    "    return anchor_box_index, rw, rh\n",
    "\n",
    "def xywh_to_ccwh(bbox):\n",
    "    x, y, w, h = bbox\n",
    "    xc, yc = x+w/2, y+h/2\n",
    "    return [xc, yc, w, h]\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ker=3, *, stride=1, padding=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, ker, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            # would have bugs if ker!=3\n",
    "            if stride != 2:\n",
    "                raise ValueError('Currently stride must be 1 or 2.')\n",
    "            self.downsampler = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "        else:\n",
    "            self.downsampler = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsampler is not None:\n",
    "            identity = self.downsampler(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def calc(x, ker=3, *, stride=1, padding=1):\n",
    "    return ((x+2*padding-ker) / stride + 1.0)//1\n",
    "\n",
    "class HW6_YOLO(nn.Module):\n",
    "\n",
    "    def __init__(self, ngf=32, size=256) -> None:\n",
    "        super().__init__()\n",
    "        # The first convolution layer. Assuing (B, 3, 256, 256) to the input.\n",
    "        model = nn.ModuleList([\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, ngf, 7, padding=0),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        # out_size: 256\n",
    "        new_size = calc(size, 7, padding=0)\n",
    "\n",
    "        # The second convolution layer, downsample only once before skip-block\n",
    "        model.extend([\n",
    "            nn.ReflectionPad2d(2),\n",
    "            nn.Conv2d(ngf, ngf * 2, 5, stride=3, padding=0),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        # out_size: 86\n",
    "        new_size = calc(new_size, 5, stride=3, padding=0)\n",
    "\n",
    "        # The skip-blocks\n",
    "        new_in_ch = ngf * 2\n",
    "        num_blocks = [6, 6, 4]\n",
    "        new_out_chs = [64, 128, 256]\n",
    "        for i in range(len(num_blocks)):\n",
    "            new_out_ch = new_out_chs[i]\n",
    "            num_block  = num_blocks[i]\n",
    "            model.extend(\n",
    "                self._gen_skip_blocks(new_in_ch, new_out_ch, num_block, stride=2, padding=1)\n",
    "            )\n",
    "            new_in_ch = new_out_ch\n",
    "            new_size = calc(new_size, 3, stride=2, padding=1)\n",
    "        # out_size: 11\n",
    "\n",
    "        model.append(nn.MaxPool2d(3, stride=2, padding=0))\n",
    "        new_size = calc(new_size, 3, stride=2, padding=0)\n",
    "        # out_size: 256x5x5\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(new_out_ch*new_size*new_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.Sigmoid(),\n",
    "            # 8 * 8 * 5 * 9 = 2880\n",
    "            nn.Linear(2048, yolo_interval**2 * 5 * 9),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_skip_blocks(in_ch, out_ch, num_layer, *, stride=1, padding=1):\n",
    "        # the first skip-block will downsample the input if necessary.\n",
    "        layers = [SkipBlock(in_ch, out_ch, stride=stride, padding=padding),]\n",
    "        for _ in range(1, num_layer):\n",
    "            # the following skip-blocks will keep the input size unchanged.\n",
    "            layers.append(SkipBlock(out_ch, out_ch, stride=1, padding=1))\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.model(x)\n",
    "        out = out.view(256*5*5, -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def train_yolo(net, data_set, batch_size=8, epoch_size=10, *, device=\"cpu\"):\n",
    "\n",
    "    net = net.to(device)\n",
    "    train_data_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    criterion1 = nn.BCELoss()\n",
    "    criterion2 = nn.MSELoss()\n",
    "    criterion3 = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-5, momentum=0.9)\n",
    "    start_time = time.perf_counter()\n",
    "    loss_record = []\n",
    "    num_anchor_box = 5\n",
    "    batch_indeces = torch.reshape(\n",
    "        torch.tensor(list(range(batch_size))*5), \n",
    "        (5,-1)\n",
    "    ).transpose(0,1).to(device)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        running_loss = 0.\n",
    "        for iter, data in enumerate(train_data_loader):\n",
    "            im_tensor, labs_tensor, yolo_vectors, cell_ids, anchor_ids = data\n",
    "            im_tensor = im_tensor.to(device)\n",
    "            labs_tensor = labs_tensor.to(device)\n",
    "            yolo_vectors = yolo_vectors.to(device)\n",
    "            cell_ids = cell_ids.to(device)\n",
    "            anchor_ids = anchor_ids.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = net(im_tensor)\n",
    "            pred_yolo_tensor = out.view(batch_size, yolo_interval**2, num_anchor_box, 9)\n",
    "\n",
    "            pred_objectness = pred_yolo_tensor[:, :, :, 0]\n",
    "            ground_objectness = torch.zeros(batch_size, yolo_interval**2, num_anchor_box)\n",
    "            # 5 is the number of objects in each image.\n",
    "\n",
    "            ground_objectness[batch_indeces, \n",
    "                             cell_ids, \n",
    "                             anchor_ids] = yolo_vectors[:,:,0]\n",
    "            loss1 = criterion1(pred_objectness, ground_objectness)\n",
    "\n",
    "            pred_yolo_vectors = pred_yolo_tensor[\n",
    "                                batch_indeces, \n",
    "                                cell_ids, \n",
    "                                anchor_ids,\n",
    "                            ][:,:,:].reshape(-1,9)\n",
    "            filter = yolo_vectors[0].nonzero(as_tuple=True)\n",
    "\n",
    "            pred_regression = pred_yolo_vectors[filter][:,1:5]\n",
    "            ground_regression = yolo_vectors[filter][:,1:5]\n",
    "            loss2 = criterion2(pred_regression, ground_regression)\n",
    "\n",
    "            pred_classification = pred_yolo_vectors[filter][:,5:]\n",
    "            ground_classification = labs_tensor[filter]\n",
    "            loss3 = criterion3(pred_classification, ground_classification)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if iter % 1024 == 1023:\n",
    "                current_time = time.perf_counter()\n",
    "                elapsed_time = current_time - start_time\n",
    "                avg_loss = running_loss / 1024\n",
    "                loss_record.append(avg_loss)\n",
    "                running_loss = 0.\n",
    "                print(\"\\n[epoch %d/%d] [iter %4d] [elapsed time %5d secs] [mean loss: %7.4f]\"\\\n",
    "                      %(epoch, epoch_size, iter, elapsed_time, avg_loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4,4)\n",
    "a[[[1,2,3],[2,3,0]],[[1,2,3],[1,2,3]]] = torch.tensor([[2,3,4],[1,1,1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# a = torch.zeros(5,5,3).float()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m a[\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m)),\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m)),\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m5\u001b[39m)))\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m----> 3\u001b[0m a[a[:,:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnonzero(as_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\u001b[39mtuple\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m))]\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# a = torch.zeros(5,5,3).float()\n",
    "a[list(range(0,3)),list(range(1,4)),0] = torch.tensor(list(range(2,5))).float()\n",
    "a[a[:,:,0].nonzero(as_tuple=True),tuple(range(0,1))]\n",
    "# a.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0.],\n",
       "        [3., 3., 0.],\n",
       "        [4., 4., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "# a[:,:,0].nonzero(as_tuple=True)\n",
    "a[a[:,:,0].nonzero(as_tuple=True)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
